---
date: "2016-04-09T16:50:16+02:00"
title: Resampling
output: 
  learnr::tutorial
weight: 2
---


We use resampling techniques for estimating model performance. In the previous section we presented the idea of fitting a model on a subset of data, known as tanning data and using the remaining sample, better known as test data, to assess the efficiency of the model. Often, this process is repeated multiple times and the results are aggregated and summarized. Hence, resampling method involves repeatedly drawing samples from a training data set and refitting a model to obtain addition information about that model. 

Resampling methods involve fitting the same statistical method multiple times using different subsets of the training data that can be computationally taxing. Fortunately, computing power has greatly advanced in the last few decades allowing for resampling techniques to become an indispensable tool of a statistical modelling procedure. 

In general, the resampling techniques differ in the way in which the subsets of data are chosen. We will consider the most commonly used resampling methods: cross-validation and bootstrap.

<p><font color="black" face="Verdana, Geneva, sans-serif" size="+1.5">**Cross-Validation: a refinement of the test set approach**</font></p>

When splitting data into training and test subset it is possible that we can end up with a test subset that may not be representative of the overall population. As a consequence, the resulting model may appear to be highly accurate when it just happened to fit well on an atypical data set and essentially it has a poor accuracy when applied to future data. 

Cross-Validation (CV), enables more assiduous accuracy checking of a model, as it is assessed on multiple different subsets of data, ensuring it will generalise well to data collected in the future. It is an extension of the training and test process that minimizes the sampling bias of machine learning models.

**K-Fold Cross-Validation**

This approach involves **randomly dividing the set of observations into _k_ groups**, known as **folds** of approximately equal size.  

With cross-validation the test data is held out (approximately one fifth of data), and the remaining training data is randomly dividing into k groups. Several different portions of this training data are used for validation, and the remaining part of data is used for training as shown in the diagram below. The k resampled estimates of performance are summarized and used for testing and model building.  

![](/module4/Resampling/images/5-fold-cv.png?width=40pc)

Since the non-holdout data is divided into five portions, we call this ‚Äú5-fold cross-validation.‚Äù If there had been ten blocks, it would have been 10-fold cross-validation. The model that has been built using k-fold cross-validation is then tested on the originally held out test data subset.  



---

**YOUR TURN** üëá

1) 

----------------------------
¬© 2020 Tatjana Kecojevic
