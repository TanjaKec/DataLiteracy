---
date: "2016-04-09T16:50:16+02:00"
title: Regression
output: 
  learnr::tutorial
weight: 3
---
Earlier we looked at how to examine sample data to investigate the nature of any relationship that may exist between a measured response variable and an attribute explanatory variable. This section examines how to investigate the nature of any relationship that may exist between a measured response variable and a measured explanatory variable. 

The first step is to have a clear idea of what is meant by a connection between the response variable and the explanatory variable. The next step is to use some **simple sample descriptive statistics** to have a first look at the nature of the link between the response variable and the explanatory variable.  This simple approach will lead to one of three conclusions, namely on the basis of the sample information:

i.	there is very strong evidence to support a link 
ii.	there is absolutely no evidence of any link 
iii.	the sample evidence is inconclusive and further more sophisticated data analysis is required

This step is called the **Initial Data Analysis** or the **IDA**.

If the IDA suggests that **Further Data Analysis** is required, then this step seeks one of two conclusions:

i.	The sample evidence is consistent with there being no link between the response variable and the explanatory variable.
ii.	The sample evidence is consistent with there being a link between the response variable and the explanatory variable.

As we have already seen in the previous sections, this process can be represented diagrammatically as:

![](/module3/DAMethodology/images/DAMethodology.png?width=40pc)

The Data-Analysis Methodology seeks to find the answer to the following key question:

- On the basis of the sample data, is there evidence of a connection/link/relationship between the response variable and the explanatory variable?

The final outcome is one of two conclusions:  

a)	There is no evidence of a relationship, labelled as the ‚ÄòNo‚Äô outcome in the diagram above, in which case the analysis is finished.

b)	There is evidence of a relationship, labelled as the ‚ÄòYes‚Äô outcome in the diagram above, in which case the nature of the relationship needs to be described.

The first step is to have a clear idea of what is meant by a connection between a measured response variable and a measured explanatory variable. Imagine a population under study consisting of a very large number of population members, and on each population member two measurements are made, the value of $Y$ the response variable and the value of $X$ the explanatory variable. For the whole population a graph of $Y$ against $X$ could be plotted conceptually. If the graph looked as in the diagram below, then there is quite clearly a link between Y and X. If the value of $X$ is known, the exact value of Y can be read off the graph. This is an unlikely scenario in the data-analysis context, because the relationship shown is a **deterministic relationship**. *Deterministic means that if the value of $X$ is known then the value of $Y$ can be precisely determined from the relationship between $Y$ and $X$.*  

![](/module3/MvM/images/deterministic_relationship.png?width=40pc)

---

**Scatter Plot**

When analysing the relationship between the two measured variable we start off by creating a scatter plot. A scatter plot is a graph that shows one axis for explanatory variable commonly known in the regression modelling as a predictor and labelled with $X$ and one axis for the response variable, which is labelled with $Y$. Thus, each point on the graph represents a single $(X, Y)$ pair. The primary benefit is that the possible relationship between the two variables can be viewed and analysed with one glance and often the nature of a relationship can be determined quickly and easily.

Let us consider a few scatter plots. The following graph represents a **perfect linear relationship**, all points lie exactly on a straight line. It is easy in this situation to determine the intercept and the slope, ie. gradient and hence specify the exact mathematical link between the response variable $Y$ and the explanatory variable $X$.

-**Graph 1**

![](/module3/MvM/images/Yes_perfect.png?width=30pc)

The relationship shown in *Graph 2* shows clearly that as the value of $X$ increases the value of $Y$ indecreases, but not exactly on a straight line as in the previous scatter plot. This is showing a statistical link, as the value of the explanatory variable $X$ increases the value of the response variable also $Y$ tends to increase. An explanation for this is that the response $Y$ may depend on a number of different variables, say  $X_1$, $X_2$, $X_3$, $X_4$, $X_5$, $X_6$ etc.  which could be written as:

$Y = f(X_1, X_2, X_3, X_4, X_5, X_6, ...)$

-**Graph 2**

![](/module3/MvM/images/Yes_50.png?width=30pc)

If the nature of the link between $Y$ and $X$ is under investigation then this could be represented as:

$$Y = f(X) + \text{effect of all other variables}$$


{{% notice note %}}
The effect of all other variables is commonly abbreviated to **e**.
{{% /notice %}}



*Graph 1* shows a link were the effect of al the other variables is nil.  The response $Y$ depends solely on the variable $X$, *Graph 2* shows a situation were $Y$ depends on $X$ but the other variables also have an influence.

Considered the model:

$$Y = f(X) + e$$
Remember üòÉ, $\text{e is the effect of all other variables}$!

The influence on the response variable $Y$ can be thought of as being made up of two components:

i. the component of $Y$ that is explained by changes in the value of $X$, [the part due to changes in $X$ through $f(X)$] 
ii.	the component of $Y$ that is explained by changes in the other factors [the part not explained by changes in $X$]

Or in more abbreviated forms:

i. the *Variation in $Y$ Explained by changes $X$* or **Explained Variation** and 
ii. the *Variation in $Y$ not explained by changes in $X$* or the **Unexplained Variation** 

The Total Variation in $Y$ are made up of the two components: 

1) *Changes in $Y$ Explained by changes in $X$*  and the 
2) *Changes in $Y$ not explained by changes in $X$* 

Which may be written as:

$$\text{The Total Variation in Y} = \text{Explained Variation} + \text{Unexplained Variation}$$

In *Graph 1* the **Unexplained Variation is nil**, since the value of $Y$ is completely determined by the value of $X$.  In *Graph 2* the **Explained Variation is large relative to the Unexplained Variation**, since the value of $Y$ is very largely influenced by the value of $X$.

Consider Graph 3.  Here there is no discernible pattern, and the value of Y seems to be unrelated to the value of X.

**- Graph 3**

![](/module3/MvM/images/No_relationship.png?width=30pc)

If $Y$ is not related to $X$ the **Explained Variation component is zero** and all the changes in $Y$ are due to the other variables, that is the **Unexplained Variation**.

Finally, consider Graphs 4 & 5 below:

-**Graph 4**

![](/module3/MvM/images/Yes_75.png?width=30pc)

-**Graph 5**

![](/module3/MvM/images/deterministic_neg.png?width=30pc)

*Graph 4* shows a similar picture to *Graph 2*, the difference being that as the value of $X$ increases the value of $Y$ decreases. The value of $Y$ is influenced by the value of $X$, so the **Explained Variation is high relative to the Unexplained Variation**.  Consider the last graph, *Graph 5*, this is a deterministic relationship, **the value of $Y$ is completely specified by the value of $X$**. Hence the **Unexplained Variation is zero**.

Graphs Summary:

|	  |Graph 1|	Graph 2|	Graph 3|	Graph 4|	Graph 5|
|---|-------|--------|---------|---------|---------|
|Explained Variation in Y|	All|	High|	Zero|	High|	All|
|Unexplained Variation in Y|	Zero|	Low|	All|	Low|	Zero|

In regression the discussion started with the following idea:
$$Y = f(X) + e$$
And to quantify the strength of the link the influence on Y was broken down into two components:
i. $$\text{The Total Variation in Y} = \text{Explained Variation} + \text{Unexplained Variation}$$

This presents two issues:

- A: Can a model of the link be made?
- B: Can **The Total Variation in Y**, **Explained Variation** and the **Unexplained Variation** be measured?

{{% notice note %}}
The simplest form of connection is a straight-line relationship and the question arises could a straight-line relationship be matched with the information contained in the graphs 1 -5?
{{% /notice %}}

-	Clearly it is easy for Graph 1 and Graph 5, the intercept and the gradient can be obtained directly from the graph. The relationship can then be written as:

$$Y = a + bX$$
where:

  i) a is the intercept and 
     
  ii) b is the gradient  

-	Since the **Explained Variation** is the same as the **Changes in Y** and the **Unexplained Variation** **is zero**, the precise evaluation of these is not necessary.

---

**Developing a Statistical Model**

For the statistical relationships as shown in *Graphs* *2* & *4* can:

a) The intercept and gradient be measured?

b) The values of the three quantities *The Total Variation in Y*, *Explained Variation* and *The   Unexplained Variation* be measured?  


It is sufficient to work out any two since:
$$\text{The Total Variation in Y} = \text{Explained Variation} + \text{Unexplained Variation}$$

Fitting a line by eye is subjective. It is unlikely that any two analysts will draw exactly the same line, hence the intercept and gradient will be slightly different from one person to the next. What is needed is an agreed method that will provide an estimate of the intercept and the gradient.  

Consider the simple numerical example below:

Suppose we would like to fit a straight-line relationship to the following data:

|$X$|	1|	2|	3|	4|	5|	6|
|---|--|---|---|---|---|---|
|$Y$|	7|	8|	12|	13|	14|	18|

The problem is to use this information to measure **the intercept** and **the gradient** for this data set.  

A simple way to do this is to draw what is considered to be **the line of best fit** by judgement or guesswork! üòÅüòâ

**The intercept** can be read off the graph as approximately $5$, and **the gradient** can be simply measured since if $X = 0$  then  $Y = 5$, and if  $X = 5$ then  $Y = 15$,  so for a change in $X$ of $5$ units (from 0 to 5) $Y$ changes by $10$, from $5$ to $15$. The definition of **the gradient** is  **The change in Y for a unit increase in X**  hence the gradient for this data set is $15/5 = 2$.

```{r, echo=FALSE}
x <- c(1:6)
y <- c(7, 8, 12, 13, 14, 18)
plot(x, y, pch = 16, main = "y = 5 + 2x + e", xlim = c(0.25, 6), ylim = c(0, 20))
abline(a = 5, b = 2, lwd = 2, lty = 2, col = 2)
```

The straight-line relationship obtained by this process is $$\hat{Y} = 5 + 2*X$$.  
Note, $\hat{Y}$ is a notation for the value of $Y$ as predicted by the straight-line relationship.

We can add more information about the predicted values of $Y$ by the *"estimated"* model to our table, so that we have

|X |Y |$\hat{Y}$  |${Y - \hat{Y}}$ |
|--|--|----------:|--------------:|
|1 |7 |7.00 | 0.00 |
|2 |8 |9.00 | -1.00|
|3 |12|11.00| 1.00|
|4 |13|13.00| 0.00|
|5 |14|15.00| -1.00|
|6 |18|17.00| 1.00|

Looking at the information contained in the previous plot, the column headed $\hat{Y}$ contains the predicted values of $Y$ for the values of $X$. For example, the first value of $\hat{Y}$ is when $X = 1$ and $Yp = 5 + 2*X$ so $\hat{Y} = 5 +2*1 = 7$.  The column headed $(Y - \hat{Y})$ is the difference between the actual value and the value predicted by the line.  For example when $X = 1$, $Y = 7$ and $\hat{Y} = 7$ so the predicted value lies on the line, as can be seen in the graph. For the value $X = 2$ the actual value lies $1$ unit below the line, again as can be seen from the graph.

The column $(Y - \hat{Y})$  **measures the disagreement between the actual data and the line** and a sensible strategy is to **make this level of disagreement as small as possible**. Referring to the graph on the table above notice that sometimes the actual data value is below the line and sometimes it is above the line, so on average the value will be close to zero. In this particular example the sum of the $(Y - \hat{Y})$ values add to zero (in a more conventional notation $\Sigma(Y - \hat{Y})  = 0$). The quantity $\Sigma(Y - \hat{Y})$ does not seem to be a satisfactory measure of disagreement, because there are a number of different lines with the property $\Sigma(Y - \hat{Y}) = 0$.

**A way of obtaining a satisfactory measure of disagreement is to square the individual $(Y - \hat{Y})$ values and add them up**.  i.e. obtain the quantity $\Sigma(Y - \hat{Y})^2$. The result is always a positive number since the square of a negative number is positive. If this quantity is then chosen to be as small as possible then the level of disagreement between the actual data points and the fitted line is the least. This provides a criterion for the choice of the best line.

The quantity $\Sigma(Y - \hat{Y})^2$ can be easily calculated and added to our table

|X |Y |$\hat{Y}$  |${Y - \hat{Y}}$ |$(Y - \hat{Y})^2$ |
|--|--|----------:|---------------:|-----------------------:|
|1 |7 |7.00 | 0.00 | 0|
|2 |8 |9.00 | -1.00| 1|
|3 |12|11.00| 1.00| 1|
|4 |13|13.00| 0.00| 0|
|5 |14|15.00| -1.00| 1|
|6 |18|17.00| 1.00| 1|

The quantity $\Sigma(Y - \hat{Y})^2$ is a measure of the disagreement between the actual $Y$ values and the values predicted by the line. If this value is chosen to be as small as possible then the disagreement between the actual Y values and the line is the smallest it could possibly be, hence the line is **The line of Best Fit**. 

This procedure of finding the intercept and the gradient of a line that makes the quantity $\Sigma(Y - \hat{Y})^2$ a minimum is called **The Method of Least Squares**. 

{{% notice info %}}
**The Method of Least Squares** was developed by [K. F Gauss (1777 - 1855)](https://link.springer.com/referenceworkentry/10.1007%2F978-1-4020-4423-6_105) a German mathematician, Gauss originated the ideas and a Russian mathematician [A. A. Markov (1856 - 1922)](https://en.wikipedia.org/wiki/Andrey_Markov) developed the method.
{{% /notice %}}

In R we use the `lm( )` function to fit linear models as illustrated below.

```{r}
x <- c(1:6)
y <- c(7, 8, 12, 13, 14, 18)
plot(x, y, pch = 16, main = "y = f(x) + e", xlim = c(0.25, 6), ylim = c(0, 20))
abline(lm(y~x), lwd = 2, lty = 2, col = 2)
```

The final issue is to find out how to measure the three quantities:

i.	The Total Variation in $Y$
ii.	The Explained Variation
iii.	The Unexplained Variation

Taking these quantities one at a time they can be measured as follows:

**The 'Unexplained Variation**
 
This turns out to be very simple to measure, the quantity: **$\Sigma(Y - \hat{Y})^2$ is a measure of the Unexplained Variation**.  If the line were a perfect fit to the data, the value predicted by the line and the actual value would be exactly the same and the value of the quantity $\Sigma(Y - \hat{Y})^2$ would be zero. This quantity is a measure of the disagreement between the actual $Y$ values and the predicted values $\hat{Y}$,which are also known as **the residuals** and are measuring the **Unexplained Variation in $Y$**. For the example used above the value of $\Sigma(Y - \hat{Y})^2$ is $3.77$.

**The Total Variation in Y**

This is related to the measures of variability (spread) introduced earlier in the course and in particular to the standard deviation ($\sigma$). To measure *The Total Variation in Y* requires a measure of spread.

*The Total Variation in Y* is defined to be the quantity: 
$\Sigma(Y - \bar{Y})^2$
Where $\bar{Y}$ is the average value of $Y$ ($\bar{Y} = \Sigma(Y)/n$).  

In our earlier example $\bar{Y} = 12$, so we can expand the table to include this calculation

|X |Y |$\hat{Y}$  |${Y - \hat{Y}}$ |$(Y - \hat{Y})^2$ | $\Sigma(Y - \bar{Y})^2$
|--|--|----------:|---------------:|-----------------------:|-------------------:|
|1 |7 |7.00 | 0.00 | 0| 25.00|
|2 |8 |9.00 | -1.00| 1| 16.00|
|3 |12|11.00| 1.00| 1| 0.00|
|4 |13|13.00| 0.00| 0| 1.00|
|5 |14|15.00| -1.00| 1| 4.00|
|6 |18|17.00| 1.00| 1| 36.00|

giving $\Sigma(Y - \bar{Y})^2 = 82$.

**The Explained Variation in Y**

If the line was a perfect fit, then the $Y$ values and the $\hat{Y}$ values would be exactly the same, and the quantity $(\hat{Y} - \bar{Y})^2$ would measure **The Total Variation in Y**. If the line is not a perfect match to the actual $Y$ values then this quantity measures **The Explained Variation in Y**.

|X |Y |$\hat{Y}$  |${Y - \hat{Y}}$ |$(Y - \hat{Y})^2$ | $\Sigma(Y - \bar{Y})^2$ | $(\hat{Y} - \bar{Y})^2$ |
|--|--|----------:|---------------:|-----------------------:|-------------------:|-----------------------:|
|1 |7 |7.00 | 0.00 | 0| 25.00| 27.94 |
|2 |8 |9.00 | -1.00| 1| 16.00| 10.06 |
|3 |12|11.00| 1.00| 1| 0.00| 1.12 |
|4 |13|13.00| 0.00| 0| 1.00| 1.12 |
|5 |14|15.00| -1.00| 1| 4.00| 10.06 |
|6 |18|17.00| 1.00| 1| 36.00| 27.94 |

incorporating this calculation into the table above will enable us to get $(\hat{Y} - \bar{Y})^2 = 78.23$.

---

**The coefficient of Determination $R^2$**








---



-----------------------------
¬© 2020 Tatjana Kecojevic
